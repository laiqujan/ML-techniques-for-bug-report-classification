{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import random\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from numpy import mean\n",
    "import time\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory usage of GPU:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**2), 'MBs')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**2), 'MBs')\n",
    "else:\n",
    "    exit(0)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start time for training\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"NLBSE_sample_10k_cleaned.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_data</th>\n",
       "      <th>issue_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>add api design document design document outlin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          issue_data  issue_label\n",
       "0  add api design document design document outlin...            0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming columns\n",
    "df = df.rename(columns={'text': 'issue_data', 'labels': 'issue_label'})\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['issue_label'] == 1, 'issue_label'] = 'valid'\n",
    "df.loc[df['issue_label'] == 0, 'issue_label'] = 'invalid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['issue_label'].value_counts())\n",
    "possible_labels = df.issue_label.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.issue_label.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing can be customized by participants\n",
    "def preprocess(row):\n",
    "  \n",
    "  # convert to string\n",
    "  doc = str(row.issue_data)\n",
    "  \n",
    "  # lowercase\n",
    "  doc = doc.lower()\n",
    "  \n",
    "  # remove punctuation\n",
    "  doc = gensim.parsing.preprocessing.strip_punctuation(doc)\n",
    "\n",
    "  # remove consecutive whitespace characters and convert tabs to spaces\n",
    "  doc = gensim.parsing.preprocessing.strip_multiple_whitespaces(doc)\n",
    "  \n",
    "  #remove stop-words\n",
    "  doc = gensim.parsing.preprocessing.remove_stopwords(doc)\n",
    "    \n",
    "  # make stems\n",
    "  doc = gensim.parsing.preprocessing.stem_text(doc)\n",
    "\n",
    "  #remove white space\n",
    "  doc = gensim.parsing.preprocessing.strip_multiple_whitespaces(doc)\n",
    "  \n",
    "  return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['issue_data'] = df.apply(preprocess, axis=1)\n",
    "\n",
    "newDF = df[['issue_label','issue_data','label']]\n",
    "df = newDF.copy()\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Load the RoBERTa tokenizer\n",
    "#tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
    "\n",
    "# Load the tokenizer\n",
    "#tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_dict), output_attentions=False,\n",
    "                                                      #output_hidden_states=False)\n",
    "\n",
    "# Load the RoBERTa model for sequence classification\n",
    "#model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_dict), output_attentions=False,\n",
    "                                                          #output_hidden_states=False)\n",
    "\n",
    "# Define the model for sequence classification\n",
    "#model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(label_dict),\n",
    "                                                      #output_attentions=False, output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def get_roc_auc_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return roc_auc_score(preds_flat, labels_flat)\n",
    "    \n",
    "def evaluate(model, dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "i = 1\n",
    "auc_scores = []\n",
    "auc_scores_1 = []\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "#get actual labels of training data\n",
    "y = df['label'].values\n",
    "for train_index, test_index in kf.split(df):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_dict), output_attentions=False,\n",
    "                                                          output_hidden_states=False)\n",
    "    print(\"Fold:\", i)\n",
    "    i=i+1\n",
    "    X_train, X_val = df.iloc[train_index], df.iloc[test_index] \n",
    "    y_train, y_val = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = X_train.index.values\n",
    "    #y_train = y_train.index.values\n",
    "    \n",
    "    X_val = X_val.index.values\n",
    "    #y_val = y_val.index.values\n",
    "    \n",
    "    df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "    df.loc[X_train, 'data_type'] = 'train'\n",
    "    df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "    #encode training data\n",
    "    encoded_data_train = tokenizer.batch_encode_plus(df[df.data_type=='train'].issue_data.values, add_special_tokens=True, \n",
    "                                                     return_attention_mask=True, padding='longest', truncation=True, return_tensors='pt')\n",
    "\n",
    "    input_ids_train = encoded_data_train['input_ids']\n",
    "    attention_masks_train = encoded_data_train['attention_mask']\n",
    "    labels_train = torch.tensor(df[df.data_type=='train'].label.values)\n",
    "\n",
    "    #encode testing data\n",
    "    encoded_data_val = tokenizer.batch_encode_plus(df[df.data_type=='val'].issue_data.values, add_special_tokens=True, \n",
    "                                                   return_attention_mask=True, padding='longest', truncation=True, return_tensors='pt')\n",
    "\n",
    "    input_ids_val = encoded_data_val['input_ids']\n",
    "    attention_masks_val = encoded_data_val['attention_mask']\n",
    "    labels_val = torch.tensor(df[df.data_type=='val'].label.values)\n",
    "\n",
    "    #prepare tensor datasets\n",
    "    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "    print(len(dataset_train), len(dataset_val))\n",
    "\n",
    "    #define batch size\n",
    "    batch_size = 8\n",
    "\n",
    "    #data loaders\n",
    "    dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "    dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)\n",
    "    \n",
    "    #define optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "    #optimizer = AdamW(model.parameters(), lr=1e-3, eps=1e-8)\n",
    "    \n",
    "    #define epochs\n",
    "    epochs = 4\n",
    "\n",
    "    #define schedualr\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)\n",
    "\n",
    "    seed_val = 17\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    \n",
    "    #model to cuda\n",
    "    model.to(device);\n",
    "    \n",
    "    best_val_loss = float('inf')  # Initialize with a very large value\n",
    "    best_epoch = 0\n",
    "    best_epoch_auc = 0.0\n",
    "    best_epoch_auc_1 = 0.0\n",
    "    \n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        model.train()\n",
    "        loss_train_total = 0\n",
    "        progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "        for batch in progress_bar:\n",
    "            model.zero_grad()\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            inputs = {'input_ids': batch[0],'attention_mask': batch[1],'labels': batch[2],}\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "        #torch.save(model.state_dict(), f'models/finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "        tqdm.write(f'\\nEpoch {epoch}')\n",
    "\n",
    "        loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "        tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "        val_loss, predictions, true_vals = evaluate(model,dataloader_validation)\n",
    "        val_f1 = f1_score_func(predictions, true_vals)\n",
    "        val_auc = get_roc_auc_func(predictions, true_vals)\n",
    "\n",
    "        tqdm.write(f'Validation loss: {val_loss}')\n",
    "        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "        tqdm.write(f'AUC: {val_auc}')\n",
    "        #Check if validation loss has improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_epoch_auc =  val_auc\n",
    "            # Save the model\n",
    "            #torch.save(model.state_dict(), f'models/best_model_finetuned_BESTDistillBERT.model')\n",
    "            #torch.save(model.state_dict(), f'models/best_model_finetuned_BESTRoberta.model')\n",
    "            #torch.save(model.state_dict(), f'models/best_model_finetuned_BESTBERT.model')\n",
    "        if best_epoch_auc_1 < val_auc:\n",
    "            best_epoch_auc_1 = val_auc\n",
    "    \n",
    "    #Print the best epoch and validation loss\n",
    "    print(f\"Best model found at epoch {best_epoch} with validation loss: {best_val_loss}\")\n",
    "    auc_scores.append(best_epoch_auc)\n",
    "    auc_scores_1.append(best_epoch_auc_1)\n",
    "    print(\"AUC of best epoch:\",best_epoch_auc)\n",
    "    print(\"Best AUC across epochs:\", best_epoch_auc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Mean AUC:\",mean(auc_scores))\n",
    "print(\"Mean of best AUC across epochs:\",mean(auc_scores_1))\n",
    "print(\"Training time in mintues:\", (time.time() - start_time)/60)\n",
    "pid = psutil.Process().pid\n",
    "memory_usage_in_bytes = psutil.Process(pid).memory_info().rss\n",
    "memory_usage_in_megabytes = memory_usage_in_bytes / 1024**2\n",
    "\n",
    "print(\"RAM in MBS:\", memory_usage_in_megabytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print('Memory usage of GPU:')\n",
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**2), 'MBs')\n",
    "print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**2), 'MBs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miq3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
